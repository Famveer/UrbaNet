{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7234ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import time\n",
    "import shutil\n",
    "import collections\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfb5f1-79ed-40bd-8db1-27180c6b16b5",
   "metadata": {},
   "source": [
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440803b0-2f0b-45a9-96ab-68da77797eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "here_path = Path().resolve()\n",
    "repo_path = here_path.parents[1]\n",
    "sys.path.append(str(repo_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce7334-8287-4b8a-b6a0-3d952d91d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.utils import verifyDir, verifyFile, verifyType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afacf91-ea4e-4c10-af2e-ecd6d8576bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.config import Config\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "np.random.seed(cfg.RANDOM_STATE)\n",
    "cfg.DATA_PATH, cfg.MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76f508-8c42-4e66-bcc5-8205d149a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "QSCORE_PATH=f\"{cfg.DATA_PATH}pp2/{cfg.SCORING_METHOD}/{cfg.PLACE_LEVEL}/\"\n",
    "IMAGES_PATH = f\"{cfg.DATA_PATH}pp2/images/\"\n",
    "MODEL_PATH = f\"{cfg.MODEL_PATH}pp2/cnn/{cfg.SCORING_METHOD}/{cfg.PLACE_LEVEL}/\"\n",
    "FEATURES_PATH = f\"{cfg.MODEL_PATH}pp2/features/{cfg.SCORING_METHOD}/{cfg.PLACE_LEVEL}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbeedf-241e-4cda-b335-c31190ad9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "verifyDir(FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db9ce3-42b7-4957-b395-c31c4540feb4",
   "metadata": {},
   "source": [
    "### Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cd3c8-ec42-4f77-8dff-b7c55db6b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_type = torch.float32 if device.type == \"cuda\" else torch.float16\n",
    "device, torch_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74de26-e62d-472c-8cfe-04341f147d07",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e45bd-3bcb-4540-b652-6e2c47a6552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 1 if \"reg\" in cfg.ML_TASK else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd546c-52f8-4169-ad87-5b54b88d49c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_df = pd.read_csv(f\"{QSCORE_PATH}scores.csv\", sep=\";\", low_memory=False)\n",
    "data_df[\"image_path\"] = f\"{IMAGES_PATH}\" + data_df[\"image_path\"]\n",
    "data_df[\"image_id\"] = data_df[\"image_id\"].apply(str)\n",
    "data_df.sort_values(by=[cfg.PERCEPTION_METRIC], ascending=False, inplace=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1da78-0506-41f1-9845-de0578dbce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.models.datasets.transformations import ImageTransforms\n",
    "\n",
    "transforms_list = ImageTransforms().get(model_name=cfg.MODEL_FEATURE_NAME)\n",
    "transforms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6227df-5bbf-4758-be26-8c9f0866dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImagesLabels(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.image_ids = dataset[\"image_id\"].tolist()\n",
    "        self.image_paths = dataset[\"image_path\"].tolist()\n",
    "        self.targets = dataset[\"target\"].tolist()\n",
    "        self.labels = dataset[\"label\"].tolist()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A single sample (image, label) where the label can be inferred from the filename or other metadata.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image_path = self.image_paths[idx]\n",
    "        image_id = self.image_ids[idx]\n",
    "        \n",
    "        # Apply any transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Example label from filename (e.g., assuming format class_index.jpg)\n",
    "        target = self.targets[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\"images_id\": image_id, \"images\": image, \"images_path\": image_path, \"targets\": target, \"labels\": label }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c095c7d-710a-41f2-a7e5-ebfa17b25573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from py.models.datasets import PlacePulse\n",
    "\n",
    "pp = PlacePulse(data_df)\n",
    "pp.DataPreparation(delta=cfg.DELTA, emotion=cfg.PERCEPTION_METRIC, city=cfg.CITY_STUDIED)\n",
    "pp.TaskPreparation(task_type=cfg.ML_TASK)\n",
    "pp.DataSplit()\n",
    "pp.DataFormat(data_formater=ImagesLabels, transforms_list=transforms_list)\n",
    "pp.DataLoader(batch_size=cfg.BATCH_SIZE, shuffle_train=False)\n",
    "pp.plot()\n",
    "\n",
    "print(f\"Train samples: {len(pp.train_df)}\")\n",
    "print(f\"Test samples: {len(pp.test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f4d53-011a-4f2e-b958-5cdea2d8c112",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd07f69-2f22-4139-82e3-32b1f6f1f843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from py.models.classification.cnn.vgg import VGG16\n",
    "\n",
    "model = VGG16(num_classes=2, use_gap=True)\n",
    "model.load_state_dict(torch.load(f\"{MODEL_PATH}{cfg.MODEL_FEATURE_NAME}_best_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ba31e-6e3d-417c-a811-b6be01f6beea",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18fcb4-5005-4c8d-941d-626a78de2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = pp.dataloaders[\"train\"]\n",
    "val_loader = pp.dataloaders[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08ba87-53d5-4275-a755-6124205be3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data_loader):\n",
    "    images_features = []\n",
    "    images_path = []\n",
    "    images_id = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        batch_images = batch['images'].to(device)\n",
    "        batch_paths = batch['images_path']\n",
    "        batch_ids = batch['images_id']\n",
    "        batch_targets = batch['targets']\n",
    "        batch_labels = batch['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = model.feature_maps(batch_images)\n",
    "            x = model.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            features = model.classifier[:-1](x)\n",
    "\n",
    "            images_features.extend(features.cpu().detach().numpy().tolist())\n",
    "            images_path.extend(batch_paths)\n",
    "            images_id.extend(batch_ids)\n",
    "            targets.extend(batch_targets.cpu().detach().numpy().tolist())\n",
    "            labels.extend(batch_labels)\n",
    "\n",
    "    return images_id, images_path, np.array(images_features).tolist(), targets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667cfce-6631-43be-86c1-c4e4dd749f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_images_id, train_images_path, train_images_features, train_targets, train_labels  = get_features(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc11946-1ef7-4206-804c-4a70159aa379",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_images_id, test_images_path, test_images_features, test_targets, test_labels  = get_features(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d467f2-ddc8-4482-8215-8804b221762c",
   "metadata": {},
   "source": [
    "### Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acebe97-b1c3-46a2-9e5f-22ee082313de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {\"train\": {\n",
    "                    \"image_id\": train_images_id,\n",
    "                    \"image_path\": train_images_path,\n",
    "                    \"features\": train_images_features,\n",
    "                    \"target\": train_targets,\n",
    "                    \"label\": train_labels,\n",
    "                }, \n",
    "                 \"test\": {\n",
    "                    \"image_id\": test_images_id,\n",
    "                    \"image_path\": test_images_path,\n",
    "                    \"features\": test_images_features,\n",
    "                    \"target\": test_targets,\n",
    "                    \"label\": test_labels,\n",
    "                }, \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea800d-1f9b-423b-be8d-e558f7b64bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "with open(f\"{FEATURES_PATH}{cfg.MODEL_FEATURE_NAME}_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
